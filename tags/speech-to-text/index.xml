<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Speech-to-Text on CYH Blog</title><link>https://jack2012aa.github.io/CYH-blog/tags/speech-to-text/</link><description>Recent content in Speech-to-Text on CYH Blog</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Mon, 07 Jul 2025 18:35:39 +0800</lastBuildDate><atom:link href="https://jack2012aa.github.io/CYH-blog/tags/speech-to-text/index.xml" rel="self" type="application/rss+xml"/><item><title>[SyntaxNext] Real-Time Speech-to-Text: Development and Choices</title><link>https://jack2012aa.github.io/CYH-blog/posts/syntaxnext/3-stt/</link><pubDate>Mon, 07 Jul 2025 18:35:39 +0800</pubDate><guid>https://jack2012aa.github.io/CYH-blog/posts/syntaxnext/3-stt/</guid><description>&lt;h1 id="apis">APIs&lt;/h1>
&lt;p>To be a vocal agent, SyntaxNext has to &amp;ldquo;listen&amp;rdquo; to what the user says. It requires a speech-to-text (STT) service. There are two types of speech-to-text: batch and real-time. Models like &lt;code>Whisper&lt;/code> support batch transcription. Users feed the model a file, and the model provides the transcript. Real-time STT receives a voice stream and returns/yields a transcript when a sentence or phrase is finished.&lt;/p>
&lt;p>Typically, batch transcription is more accurate than real-time transcription. But in a vocal agent situation, latency may be more important than accuracy, since the agent LLM can often infer the correct meaning from a slightly inaccurate transcript, and the precision is generally quite good.&lt;/p></description></item></channel></rss>