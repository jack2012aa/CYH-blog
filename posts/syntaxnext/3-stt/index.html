<!doctype html><html class="not-ready lg:text-base" style=--bg:#faf8f1 lang=en-us dir=ltr><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>[SyntaxNext] Real-Time Speech-to-Text: Development and Choices - CYH Blog</title>
<meta name=theme-color><meta name=description content="APIs
To be a vocal agent, SyntaxNext has to &ldquo;listen&rdquo; to what the user says. It requires a speech-to-text (STT) service. There are two types of speech-to-text: batch and real-time. Models like Whisper support batch transcription. Users feed the model a file, and the model provides the transcript. Real-time STT receives a voice stream and returns/yields a transcript when a sentence or phrase is finished.
Typically, batch transcription is more accurate than real-time transcription. But in a vocal agent situation, latency may be more important than accuracy, since the agent LLM can often infer the correct meaning from a slightly inaccurate transcript, and the precision is generally quite good."><meta name=author content="Chang-Yu Huang"><link rel="preload stylesheet" as=style href=https://jack2012aa.github.io/CYH-blog/main.min.css><link rel=preload as=image href=https://jack2012aa.github.io/CYH-blog/theme.png><link rel=preload as=image href=img/oct.JPG><link rel=preload as=image href=https://jack2012aa.github.io/CYH-blog/github.svg><script defer src=https://jack2012aa.github.io/CYH-blog/highlight.min.js onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://jack2012aa.github.io/CYH-blog/img/cloud.JPG><link rel=apple-touch-icon href=https://jack2012aa.github.io/CYH-blog/apple-touch-icon.png><meta name=generator content="Hugo 0.139.3"><meta itemprop=name content="[SyntaxNext] Real-Time Speech-to-Text: Development and Choices"><meta itemprop=description content="APIs To be a vocal agent, SyntaxNext has to “listen” to what the user says. It requires a speech-to-text (STT) service. There are two types of speech-to-text: batch and real-time. Models like Whisper support batch transcription. Users feed the model a file, and the model provides the transcript. Real-time STT receives a voice stream and returns/yields a transcript when a sentence or phrase is finished.
Typically, batch transcription is more accurate than real-time transcription. But in a vocal agent situation, latency may be more important than accuracy, since the agent LLM can often infer the correct meaning from a slightly inaccurate transcript, and the precision is generally quite good."><meta itemprop=datePublished content="2025-07-07T18:35:39+08:00"><meta itemprop=dateModified content="2025-07-07T18:35:39+08:00"><meta itemprop=wordCount content="497"><meta itemprop=keywords content="Python,Speech-to-Text"><meta property="og:url" content="https://jack2012aa.github.io/CYH-blog/posts/syntaxnext/3-stt/"><meta property="og:site_name" content="CYH Blog"><meta property="og:title" content="[SyntaxNext] Real-Time Speech-to-Text: Development and Choices"><meta property="og:description" content="APIs To be a vocal agent, SyntaxNext has to “listen” to what the user says. It requires a speech-to-text (STT) service. There are two types of speech-to-text: batch and real-time. Models like Whisper support batch transcription. Users feed the model a file, and the model provides the transcript. Real-time STT receives a voice stream and returns/yields a transcript when a sentence or phrase is finished.
Typically, batch transcription is more accurate than real-time transcription. But in a vocal agent situation, latency may be more important than accuracy, since the agent LLM can often infer the correct meaning from a slightly inaccurate transcript, and the precision is generally quite good."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-07-07T18:35:39+08:00"><meta property="article:modified_time" content="2025-07-07T18:35:39+08:00"><meta property="article:tag" content="Python"><meta property="article:tag" content="Speech-to-Text"><meta name=twitter:card content="summary"><meta name=twitter:title content="[SyntaxNext] Real-Time Speech-to-Text: Development and Choices"><meta name=twitter:description content="APIs To be a vocal agent, SyntaxNext has to “listen” to what the user says. It requires a speech-to-text (STT) service. There are two types of speech-to-text: batch and real-time. Models like Whisper support batch transcription. Users feed the model a file, and the model provides the transcript. Real-time STT receives a voice stream and returns/yields a transcript when a sentence or phrase is finished.
Typically, batch transcription is more accurate than real-time transcription. But in a vocal agent situation, latency may be more important than accuracy, since the agent LLM can often infer the correct meaning from a slightly inaccurate transcript, and the precision is generally quite good."><link rel=canonical href=https://jack2012aa.github.io/CYH-blog/posts/syntaxnext/3-stt/></head><body class="text-black duration-200 ease-out dark:text-white"><header class="mx-auto flex h-[4.5rem] max-w-[--w] px-8 lg:justify-center"><div class="relative z-50 ltr:mr-auto rtl:ml-auto flex items-center"><a class="-translate-y-[1px] text-2xl font-medium" href=https://jack2012aa.github.io/CYH-blog/>CYH Blog</a><div class="btn-dark text-[0] ltr:ml-4 rtl:mr-4 h-6 w-6 shrink-0 cursor-pointer [background:url(./theme.png)_left_center/_auto_theme('spacing.6')_no-repeat] [transition:_background-position_0.4s_steps(5)] dark:[background-position:right]" role=button aria-label=Dark></div></div><div class="btn-menu relative z-50 ltr:-mr-8 rtl:-ml-8 flex h-[4.5rem] w-[5rem] shrink-0 cursor-pointer flex-col items-center justify-center gap-2.5 lg:hidden" role=button aria-label=Menu></div><script>const htmlClass=document.documentElement.classList;setTimeout(()=>{htmlClass.remove("not-ready")},10);const btnMenu=document.querySelector(".btn-menu");btnMenu.addEventListener("click",()=>{htmlClass.toggle("open")});const metaTheme=document.querySelector('meta[name="theme-color"]'),lightBg="#faf8f1".replace(/"/g,""),setDark=e=>{metaTheme.setAttribute("content",e?"#000":lightBg),htmlClass[e?"add":"remove"]("dark"),localStorage.setItem("dark",e)},darkScheme=window.matchMedia("(prefers-color-scheme: dark)");if(htmlClass.contains("dark"))setDark(!0);else{const e=localStorage.getItem("dark");setDark(e?e==="true":darkScheme.matches)}darkScheme.addEventListener("change",e=>{setDark(e.matches)});const btnDark=document.querySelector(".btn-dark");btnDark.addEventListener("click",()=>{setDark(localStorage.getItem("dark")!=="true")})</script><div class="nav-wrapper fixed inset-x-0 top-full z-40 flex h-full select-none flex-col justify-center pb-16 duration-200 dark:bg-black lg:static lg:h-auto lg:flex-row lg:!bg-transparent lg:pb-0 lg:transition-none"><nav class="lg:ml-12 lg:flex lg:flex-row lg:items-center lg:space-x-10 rtl:space-x-reverse"><a class="block text-center text-xl leading-[5rem] lg:text-base lg:font-normal" href=/CYH-blog/about/>About</a>
<a class="block text-center text-xl leading-[5rem] lg:text-base lg:font-normal" href=/CYH-blog/categories/courseworks/>Courseworks</a>
<a class="block text-center text-xl leading-[5rem] lg:text-base lg:font-normal" href=/CYH-blog/categories/sideprojects/>Side Projects</a></nav><nav class="mt-12 flex justify-center space-x-10 rtl:space-x-reverse dark:invert ltr:lg:ml-14 rtl:lg:mr-14 lg:mt-0 lg:items-center"><a class="h-7 w-7 text-[0] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6" style=--url:url(./github.svg) href=https://github.com/jack2012aa target=_blank rel=me>github</a></nav></div></header><main class="prose prose-neutral relative mx-auto min-h-[calc(100vh-9rem)] max-w-[--w] px-8 pb-16 pt-14 dark:prose-invert"><article><header class=mb-14><h1 class="!my-0 pb-2.5">[SyntaxNext] Real-Time Speech-to-Text: Development and Choices</h1><div class="text-xs antialiased opacity-60"><time>Jul 7, 2025</time></div></header><section><h1 id=apis>APIs</h1><p>To be a vocal agent, SyntaxNext has to &ldquo;listen&rdquo; to what the user says. It requires a speech-to-text (STT) service. There are two types of speech-to-text: batch and real-time. Models like <code>Whisper</code> support batch transcription. Users feed the model a file, and the model provides the transcript. Real-time STT receives a voice stream and returns/yields a transcript when a sentence or phrase is finished.</p><p>Typically, batch transcription is more accurate than real-time transcription. But in a vocal agent situation, latency may be more important than accuracy, since the agent LLM can often infer the correct meaning from a slightly inaccurate transcript, and the precision is generally quite good.</p><p>Besides accuracy and latency, different service providers offer different interfaces for real-time STT. In our case, we need this information to control the UI:</p><ol><li>when the user starts speaking,</li><li>when the user stops speaking,</li><li>the final transcript.</li></ol><p>Other commonly provided information includes:</p><ol start=4><li>the partial transcript,</li><li>other possible transcripts and their probability of being correct.</li></ol><p>Along with some control methods, we can define a list of APIs. Most of the mentioned information represents some kind of event. Hence, the API can be designed to be event-based.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>set_language</span>(language) <span style=color:#f92672>-&gt;</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>allow_partial_transcript</span>(is_allowed) <span style=color:#f92672>-&gt;</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>set_VAD_threshold</span>(threshold) <span style=color:#f92672>-&gt;</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>set_sentence_finishing_duration</span>(time) <span style=color:#f92672>-&gt;</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>set_stream_encode</span>(encode) <span style=color:#f92672>-&gt;</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>set_stream_sampling_rate</span>(rate) <span style=color:#f92672>-&gt;</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>start_streaming</span>(stream) <span style=color:#f92672>-&gt;</span> asyncio<span style=color:#f92672>.</span>Queue[Event]
</span></span></code></pre></div><h1 id=real-time-to-batch>Real-Time to Batch</h1><p>There are two ways to implement the API. The first way is to depend entirely on the model, letting it judge whether the user is speaking and what they said. Another way, similar to <a href=https://github.com/KoljaB/RealtimeSTT/tree/master>RealtimeSTT</a>, is to first determine if the user is speaking. If they are, the library starts recording the voice and feeds it to the model after the user finishes speaking.</p><p>Both ways consume a lot of computational resources because <strong>they create a model for each stream.</strong> The speed of batch transcription is fast, taking only about 10% of the voice&rsquo;s duration. If we can segment the stream into periods where we are sure there is sound, performance can be improved by up to 90%.</p><p>The two most popular voice activity detection (VAD) libraries are <code>WebRTC VAD</code> and <code>silero_vad</code>. The former uses statistics, and the latter is model-based. Both are lightweight. The best practice is to record periods where the VAD detects voice activity and then send that audio to the backend batch transcription service. This practice not only saves bandwidth by not transporting the entire stream but also moves the VAD computation to the client side.</p><p>However, I couldn&rsquo;t find any library that I could use in a browser. I didn&rsquo;t try compiling them into WebAssembly, though it might work. Nevertheless, even passing the entire stream using WebSocket still provides excellent performance and accuracy.</p><h1 id=cost>Cost</h1><p>Although the result is good, another consideration is cost. We don&rsquo;t have many users right now. According to my calculations, using our own service is worthwhile only when we have more than 10 daily users. But I think this architecture is still promising.</p></section><footer class="mt-12 flex flex-wrap"><a class="mb-1.5 ltr:mr-1.5 rtl:ml-1.5 rounded-lg bg-black/[3%] px-5 py-1 no-underline hover:bg-black/[6%] dark:bg-white/[8%] dark:hover:bg-white/[12%]" href=https://jack2012aa.github.io/CYH-blog/tags/python>Python</a>
<a class="mb-1.5 ltr:mr-1.5 rtl:ml-1.5 rounded-lg bg-black/[3%] px-5 py-1 no-underline hover:bg-black/[6%] dark:bg-white/[8%] dark:hover:bg-white/[12%]" href=https://jack2012aa.github.io/CYH-blog/tags/speech-to-text>Speech-To-Text</a></footer><nav class="mt-24 flex overflow-hidden rounded-xl bg-black/[3%] text-lg !leading-[1.2] *:flex *:w-1/2 *:items-center *:p-5 *:font-medium *:no-underline dark:bg-white/[8%] [&>*:hover]:bg-black/[2%] dark:[&>*:hover]:bg-white/[3%]"><a class="ltr:ml-auto rtl:mr-auto justify-end pl-3" href=https://jack2012aa.github.io/CYH-blog/posts/syntaxnext/2-react/><span>[SyntaxNext] How to Combine React and Traditional TypeScript</span><span class="ltr:ml-1.5 rtl:mr-1.5">→</span></a></nav></article></main><footer class="mx-auto flex h-[4.5rem] max-w-[--w] items-center px-8 text-xs uppercase tracking-wider opacity-60"><div class=mr-auto>&copy; 2025
<a class=link href=https://jack2012aa.github.io/CYH-blog/>CYH Blog</a></div><a class="link mx-6" href=https://gohugo.io/ rel=noopener target=_blank>powered by hugo️️</a>️
<a class=link href=https://github.com/nanxiaobei/hugo-paper rel=noopener target=_blank>hugo-paper</a></footer></body></html>