<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>SideProjects on CYH Blog</title><link>https://jack2012aa.github.io/CYH-blog/categories/sideprojects/</link><description>Recent content in SideProjects on CYH Blog</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Mon, 07 Jul 2025 18:35:39 +0800</lastBuildDate><atom:link href="https://jack2012aa.github.io/CYH-blog/categories/sideprojects/index.xml" rel="self" type="application/rss+xml"/><item><title>[SyntaxNext] Real-Time Speech-to-Text: Development and Choices</title><link>https://jack2012aa.github.io/CYH-blog/posts/syntaxnext/3-stt/</link><pubDate>Mon, 07 Jul 2025 18:35:39 +0800</pubDate><guid>https://jack2012aa.github.io/CYH-blog/posts/syntaxnext/3-stt/</guid><description>&lt;h1 id="apis">APIs&lt;/h1>
&lt;p>To be a vocal agent, SyntaxNext has to &amp;ldquo;listen&amp;rdquo; to what the user says. It requires a speech-to-text (STT) service. There are two types of speech-to-text: batch and real-time. Models like &lt;code>Whisper&lt;/code> support batch transcription. Users feed the model a file, and the model provides the transcript. Real-time STT receives a voice stream and returns/yields a transcript when a sentence or phrase is finished.&lt;/p>
&lt;p>Typically, batch transcription is more accurate than real-time transcription. But in a vocal agent situation, latency may be more important than accuracy, since the agent LLM can often infer the correct meaning from a slightly inaccurate transcript, and the precision is generally quite good.&lt;/p></description></item><item><title>[SyntaxNext] How to Combine React and Traditional TypeScript</title><link>https://jack2012aa.github.io/CYH-blog/posts/syntaxnext/2-react/</link><pubDate>Mon, 07 Jul 2025 18:35:31 +0800</pubDate><guid>https://jack2012aa.github.io/CYH-blog/posts/syntaxnext/2-react/</guid><description>&lt;h1 id="initial-design">Initial Design&lt;/h1>
&lt;p>At the beginning, we used a pure HTML, CSS, and TypeScript architecture. This was a logical decision, as less than 40% of the extension&amp;rsquo;s code is UI-related, and none of us were familiar with frontend frameworks. This approach also avoided the added complexity of state management libraries.&lt;/p>
&lt;p>In this design, the file running as the content script (which we called &lt;code>content&lt;/code>) imports other files to perform its tasks, including the UI component (which we called &lt;code>interviewTab&lt;/code>). &lt;code>interviewTab&lt;/code> exports functions that change the UI&amp;rsquo;s appearance, and the state is managed in &lt;code>content&lt;/code>. This worked well when the UI was simple and indeed shortened development time.&lt;/p></description></item><item><title>[SyntaxNext] Project Introduction</title><link>https://jack2012aa.github.io/CYH-blog/posts/syntaxnext/1-intro/</link><pubDate>Mon, 07 Jul 2025 18:27:11 +0800</pubDate><guid>https://jack2012aa.github.io/CYH-blog/posts/syntaxnext/1-intro/</guid><description>&lt;p>Hi everyone. This is a new series about what I learned while developing our side project, SyntaxNext.&lt;/p>
&lt;h1 id="what-syntaxnext-does">What SyntaxNext Does&lt;/h1>
&lt;p>SyntaxNext is an AI-powered mock interview agent. It works as a Chrome extension, and users can choose any LeetCode problem. During the interview, the extension shows the stages and tasks that an interviewee should complete. The agent will describe the question, and users should tell the agent how they plan to solve the problem. Users can go to the next step only when the agent thinks they have finished the required tasks in the current step.&lt;/p></description></item></channel></rss>